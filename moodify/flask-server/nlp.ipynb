{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embeddings_df = pd.read_csv('master_dataset.csv')\n",
    "vector_embeddings_df['tokens'] = vector_embeddings_df['tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences = vector_embeddings_df['tokens'], vector_size = 50, window = 5, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_embeddings(tokens):\n",
    "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    embeddings_array = np.array(embeddings) # UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.\n",
    "    return torch.tensor(embeddings_array)\n",
    "\n",
    "vector_embeddings_df['embeddings'] = vector_embeddings_df['tokens'].apply(apply_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "vectors = vector_embeddings_df['embeddings'].tolist()\n",
    "torch_padded_tensor = pad_sequence([torch.FloatTensor(np.array(vector)) for vector in vectors], batch_first = True) # Convert each list of embeddings to a FloatTensor and pad them\n",
    "print(torch_padded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(tensor):\n",
    "  return torch.mean(tensor, dim = 0).numpy()\n",
    "\n",
    "mean_embeddings = vector_embeddings_df['embeddings'].apply(calculate_mean)\n",
    "mean_embeddings_df = pd.DataFrame(mean_embeddings.tolist(), columns=[f'embedding_{i}' for i in range(mean_embeddings.iloc[0].shape[0])])\n",
    "mean_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embeddings_df['cleaned_lyrics'] = vector_embeddings_df['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 84)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(vector_embeddings_df['cleaned_text']) # Fit and transform the text data to get the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_counts = tfidf_matrix.sum(axis = 0) # Sum the TF-IDF values across all documents for each word\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out() # Get the feature names (words)\n",
    "word_freq_df = pd.DataFrame({'Word': feature_names, 'Frequency': np.squeeze(np.asarray(word_freq_counts))})\n",
    "word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False) # Frequency in descending order\n",
    "print(word_freq_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(word_freq_df['Word'], word_freq_df['Frequency'], color='skyblue')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Frequency Counts')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = pd.concat([vector_embeddings_df, tfidf_df, mean_embeddings_df], axis=1)\n",
    "mood_list = combined_features['mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['track_names', 'first_artists', 'lyrics', 'tokens', 'mood','embeddings', 'cleaned_text', 'acousticness', 'mode', 'instrumentalness']\n",
    "combined_features = combined_features.drop(columns=columns_to_drop)\n",
    "print(combined_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create feature and label sets:\n",
    "\n",
    "vector_embeddings_df = pd.read_csv('master_dataset.csv')\n",
    "vector_embeddings_df['tokens'] = vector_embeddings_df['tokens'].apply(ast.literal_eval)\n",
    "vector_embeddings_df['cleaned_lyrics'] = vector_embeddings_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "audio_features = ['danceability', 'energy', 'loudness', 'mode', 'acousticness', 'instrumentalness', 'valence', 'tempo']\n",
    "X_audio = vector_embeddings_df[audio_features]\n",
    "y = vector_embeddings_df['mood'].map({'Happy': 1, 'Sad': 0})\n",
    "lyrics = vector_embeddings_df['cleaned_lyrics']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_audio_scaled = scaler.fit_transform(X_audio)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_lyrics(lyrics):\n",
    "    inputs = tokenizer(lyrics, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = bert_model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "X_lyrics = np.vstack([encode_lyrics(lyric) for lyric in lyrics])\n",
    "\n",
    "X_combined = np.hstack((X_audio_scaled, X_lyrics))\n",
    "\n",
    "y = np.array([1 if mood == 'Happy' else 0 for mood in mood_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "# Pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid_rf = {\n",
    "    'rf__min_samples_leaf': [8, 10, 12],\n",
    "    'rf__max_depth': list(range(2, 6)),\n",
    "    'rf__max_features': [\"sqrt\", \"log2\"]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv = 5, n_jobs = -1, scoring = 'accuracy')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    grid_search_rf.fit(X_combined, y)\n",
    "\n",
    "print(grid_search_rf.best_params_)\n",
    "print(\"Accuracy: \", grid_search_rf.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_clf = MLPClassifier()\n",
    "\n",
    "# Param grid\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(mlp_clf, param_grid_mlp, cv = 10, n_jobs = -1, scoring = 'accuracy')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    grid_search_mlp.fit(X_combined, y)\n",
    "\n",
    "print(grid_search_mlp.best_params_)\n",
    "print(\"Accuracy: \", grid_search_mlp.best_score_ * 100)\n",
    "best_estimator = grid_search_mlp.best_estimator_\n",
    "y_pred = best_estimator.predict(X_combined)\n",
    "print(\"Prediction List:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "master_df = pd.read_csv('master_dataset.csv')\n",
    "y = master_df['mood'].map({'Happy': 1, 'Sad': 0})\n",
    "columns_to_drop = [\"track_names\", \"first_artists\", \"tokens\", \"embeddings\", \"danceability\", \"energy\", \"loudness\", \"mode\", \"acousticness\", \"instrumentalness\", \"valence\", \"tempo\"]\n",
    "cleaned_df = master_df.drop(columns=columns_to_drop)\n",
    "X = cleaned_df['lyrics']\n",
    "\n",
    "# Tokenize and encode lyrics\n",
    "encoded_data = tokenizer(X.tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "# Split data\n",
    "X_train_ids, X_val_ids, y_train, y_val, X_train_mask, X_val_mask = train_test_split(input_ids, y, attention_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "# Move model to GPU (I was using Colab)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_data = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 10  # We are currently using 10 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_train_loss}')\n",
    "    \n",
    "    gc.collect() # Clear cache and collect garbage due to Colab limits\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "model.save_pretrained('fine_tuned_roberta')\n",
    "model.eval()\n",
    "val_data = TensorDataset(X_val_ids, X_val_mask, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=8)\n",
    "val_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data:\n",
    "new_lyrics = \"Your new lyrics here...\"\n",
    "encoded_new_lyrics = tokenizer(new_lyrics, padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "with torch.no_grad():\n",
    "    new_lyrics_preds = model(input_ids=encoded_new_lyrics['input_ids'], attention_mask=encoded_new_lyrics['attention_mask'])\n",
    "    predicted_mood = torch.argmax(new_lyrics_preds.logits).item()\n",
    "    mood_label = \"Happy\" if predicted_mood == 0 else \"Sad\"\n",
    "    print(f\"Predicted mood: {mood_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
